
Every now and then we are asked whether PsN is validated. The term "validation" is used differently by different people, so we cannot answer that question with a simple yes or no.

Any pharmacometrics software must be used with care, knowing the limitations of the tool. For example, NONMEM can give message "MINIMIZATION SUCCESSFUL" also for local minima, and just because the ofv drops with 3.84 it does not necessarily mean that adding a parameter was appropriate. The user must look at the evidence in combination. Similarly, when using PsN the user must understand the implemented methods and their limitations.  

PsN is extensively tested. We have an ever growing test suite that we run with several combinations of NONMEM versions, Perl versions, Fortran compilers and operating systems. The test suite is included in the installation package and can be used by anyone. All new code that is written has a set of tests. When bugs are discovered we both fix them and add new tests to the test suite to ensure that those particular bugs cannot reappear. We do not have tests for every subroutine of PsN, and that is especially true for old code. A strength of PsN is that there is a very large group of users who check results and report back any problems to us. Many users have their own validation suites and tell us when they find issues. Bugs that have not yet been found by us are likely to be discovered by someone else.  Known bugs are published to the known-bugs-wiki of PsN, so it is easy to keep track of what other users discover.

If we focus on the issue of "confirming that the software is doing what it says to do" then the answer is definitely yes for large parts of PsN, but not for all. We like to call it "test coverage". In the 4.4.6 release we have excellent test coverage for execute (parsing NONMEM control streams, running models, tweaking initial estimates, and reading of final estimates from NONMEM output and reporting them in the raw_results file). The same well tested code for running NONMEM is used by all other tools. Tests are also excellent for tool-specific features of precond, sir, parallel_retries, randtest, sse and sumo. For vpc we have good coverage for the basic vpc features (extracting numbers from NONMEM tables, binning data and computing intervals without censoring or prediction correction), but we have only very basic crash tests for the more advanced features. bootstrap is well tested up until the creation of the raw results file, but we are lacking tests for checking that the reported confidence intervals etc in bootstrap_results.csv are correct given the results in raw_results. scm has many crash tests, and tests to ensure that the computed means/medians/max/min of covariates are correct, but not that the generated control streams are what they should be. 

When deciding whether to use PsN or not, we believe that the relevant question to ask is what the alternative to PsN is. If the alternative is another software where every line of code is well tested and every algorithm is has been proven to provide the desired result, then that other software is preferred. If however the alternative is that the pharmacometrician performs, for example, less model diagnostics due to the lack of a software that offers the relevant functionality, or has to write his/her own scripts to perform the necessary tasks, then it is likely that using PsN will both save time and increase the quality of the work.

